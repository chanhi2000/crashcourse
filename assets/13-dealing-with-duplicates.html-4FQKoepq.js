import{_ as h}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as b,a as e,t as v,b as i,w as n,n as k,g,d as s,e as d,r,o as _}from"./app-U_bew1in.js";const f={},w={id:"frontmatter-title-관련",tabindex:"-1"},x={class:"header-anchor",href:"#frontmatter-title-관련"},y={class:"table-of-contents"},A=e("hr",null,null,-1),C=e("hr",null,null,-1),$=e("p",null,[s("Often, you need to eliminate duplicates from an input file. This could be based on the entire line content or based on certain fields. These are typically solved with the "),e("code",null,"sort"),s(" and "),e("code",null,"uniq"),s(" commands. Advantages with "),e("code",null,"awk"),s(" include regexp based field and record separators, input doesn't have to be sorted, and in general more flexibility because it is a programming language.")],-1),F={class:"hint-container info"},N=e("p",{class:"hint-container-title"},"Info",-1),T={href:"https://github.com/learnbyexample/learn_gnuawk/tree/master/example_files",target:"_blank",rel:"noopener noreferrer"},j=d(`<hr><h2 id="whole-line-duplicates" tabindex="-1"><a class="header-anchor" href="#whole-line-duplicates"><span>Whole line duplicates</span></a></h2><p><code>awk &#39;!a[$0]++&#39;</code> is one of the most famous <code>awk</code> one-liners. It eliminates line based duplicates while retaining the input order. The following example shows it in action along with an illustration of how the logic works.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line"><span class="token function">cat</span> purchases.txt</span>
<span class="line"><span class="token comment"># coffee</span></span>
<span class="line"><span class="token comment"># tea</span></span>
<span class="line"><span class="token comment"># washing powder</span></span>
<span class="line"><span class="token comment"># coffee</span></span>
<span class="line"><span class="token comment"># toothpaste</span></span>
<span class="line"><span class="token comment"># tea</span></span>
<span class="line"><span class="token comment"># soap</span></span>
<span class="line"><span class="token comment"># tea</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,4),S=e("div",{class:"language-bash line-numbers-mode","data-highlighter":"prismjs","data-ext":"sh","data-title":"sh"},[e("pre",null,[e("code",null,[e("span",{class:"line"},[e("span",{class:"token function"},"awk"),s(),e("span",{class:"token string"},`'{print +a[$0] "\\t" $0; a[$0]++}'`),s(" purchases.txt")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# 0       coffee")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# 0       tea")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# 0       washing powder")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# 1       coffee")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# 0       toothpaste")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# 1       tea")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# 0       soap")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# 2       tea")]),s(`
`),e("span",{class:"line"})])]),e("div",{class:"line-numbers","aria-hidden":"true",style:{"counter-reset":"line-number 0"}},[e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"})])],-1),E=e("p",null,"only those entries with zero in first column will be retained",-1),R=e("div",{class:"language-bash line-numbers-mode","data-highlighter":"prismjs","data-ext":"sh","data-title":"sh"},[e("pre",null,[e("code",null,[e("span",{class:"line"},[e("span",{class:"token function"},"awk"),s(),e("span",{class:"token string"},"'!a[$0]++'"),s(" purchases.txt")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# coffee")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# tea")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# washing powder")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# toothpaste")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# soap")]),s(`
`),e("span",{class:"line"})])]),e("div",{class:"line-numbers","aria-hidden":"true",style:{"counter-reset":"line-number 0"}},[e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"})])],-1),D={class:"hint-container info"},q=e("p",{class:"hint-container-title"},"Info",-1),I={href:"https://github.com/koraa/huniq",target:"_blank",rel:"noopener noreferrer"},P=d(`<hr><h2 id="column-wise-duplicates" tabindex="-1"><a class="header-anchor" href="#column-wise-duplicates"><span>Column wise duplicates</span></a></h2><p>Removing field based duplicates is simple for a single field comparison. Just change $0 to the required field number after setting the appropriate field separator.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line"><span class="token function">cat</span> duplicates.txt</span>
<span class="line"><span class="token comment"># brown,toy,bread,42</span></span>
<span class="line"><span class="token comment"># dark red,ruby,rose,111</span></span>
<span class="line"><span class="token comment"># blue,ruby,water,333</span></span>
<span class="line"><span class="token comment"># dark red,sky,rose,555</span></span>
<span class="line"><span class="token comment"># yellow,toy,flower,333</span></span>
<span class="line"><span class="token comment"># white,sky,bread,111</span></span>
<span class="line"><span class="token comment"># light red,purse,rose,333</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,4),G=e("p",null,"based on the last field",-1),Q=e("div",{class:"language-bash line-numbers-mode","data-highlighter":"prismjs","data-ext":"sh","data-title":"sh"},[e("pre",null,[e("code",null,[e("span",{class:"line"},[e("span",{class:"token function"},"awk"),s(" -F, "),e("span",{class:"token string"},"'!seen[$NF]++'"),s(" duplicates.txt")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# brown,toy,bread,42")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# dark red,ruby,rose,111")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# blue,ruby,water,333")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# dark red,sky,rose,555")]),s(`
`),e("span",{class:"line"})])]),e("div",{class:"line-numbers","aria-hidden":"true",style:{"counter-reset":"line-number 0"}},[e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"})])],-1),U=e("p",null,[s("For multiple fields comparison, separate the fields with "),e("code",null,","),s(" so that "),e("code",null,"SUBSEP"),s(" is used to combine the field values to generate the key. As mentioned before, "),e("code",null,"SUBSEP"),s(" has a default value of "),e("code",null,"\\034"),s(" non-printing character, which is typically not used in text files.")],-1),V=e("p",null,"based on the first and third fields",-1),B=e("div",{class:"language-bash line-numbers-mode","data-highlighter":"prismjs","data-ext":"sh","data-title":"sh"},[e("pre",null,[e("code",null,[e("span",{class:"line"},[e("span",{class:"token function"},"awk"),s(" -F, "),e("span",{class:"token string"},"'!seen[$1,$3]++'"),s(" duplicates.txt")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# brown,toy,bread,42")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# dark red,ruby,rose,111")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# blue,ruby,water,333")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# yellow,toy,flower,333")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# white,sky,bread,111")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# light red,purse,rose,333")]),s(`
`),e("span",{class:"line"})])]),e("div",{class:"line-numbers","aria-hidden":"true",style:{"counter-reset":"line-number 0"}},[e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"})])],-1),L=e("hr",null,null,-1),z=e("h2",{id:"duplicate-count",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#duplicate-count"},[e("span",null,"Duplicate count")])],-1),O=e("p",null,"In this section, how many times a duplicate record is found plays a role in determining the output.",-1),W=e("p",null,"First up, printing only a specific numbered duplicate.",-1),K=e("p",null,"print only the second occurrence of duplicates based on the second field",-1),Z=e("div",{class:"language-bash line-numbers-mode","data-highlighter":"prismjs","data-ext":"sh","data-title":"sh"},[e("pre",null,[e("code",null,[e("span",{class:"line"},[e("span",{class:"token function"},"awk"),s(" -F, "),e("span",{class:"token string"},"'++seen[$2]==2'"),s(" duplicates.txt")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# blue,ruby,water,333")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# yellow,toy,flower,333")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# white,sky,bread,111")]),s(`
`),e("span",{class:"line"})])]),e("div",{class:"line-numbers","aria-hidden":"true",style:{"counter-reset":"line-number 0"}},[e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"})])],-1),J=e("p",null,"print only the third occurrence of duplicates based on the last field",-1),M=e("div",{class:"language-bash line-numbers-mode","data-highlighter":"prismjs","data-ext":"sh","data-title":"sh"},[e("pre",null,[e("code",null,[e("span",{class:"line"},[e("span",{class:"token function"},"awk"),s(" -F, "),e("span",{class:"token string"},"'++seen[$NF]==3'"),s(" duplicates.txt")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# light red,purse,rose,333")]),s(`
`),e("span",{class:"line"})])]),e("div",{class:"line-numbers","aria-hidden":"true",style:{"counter-reset":"line-number 0"}},[e("div",{class:"line-number"}),e("div",{class:"line-number"})])],-1),H=e("p",null,[s("Next, printing only the last copy of duplicates. Since the count isn't known, the "),e("code",null,"tac"),s(" command comes in handy again.")],-1),X=e("p",null,"reverse the input line-wise, retain first copy and then reverse again",-1),Y=e("div",{class:"language-bash line-numbers-mode","data-highlighter":"prismjs","data-ext":"sh","data-title":"sh"},[e("pre",null,[e("code",null,[e("span",{class:"line"},[e("span",{class:"token function"},"tac"),s(" duplicates.txt "),e("span",{class:"token operator"},"|"),s(),e("span",{class:"token function"},"awk"),s(" -F, "),e("span",{class:"token string"},"'!seen[$NF]++'"),s(),e("span",{class:"token operator"},"|"),s(),e("span",{class:"token function"},"tac")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# brown,toy,bread,42")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# dark red,sky,rose,555")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# white,sky,bread,111")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# light red,purse,rose,333")]),s(`
`),e("span",{class:"line"})])]),e("div",{class:"line-numbers","aria-hidden":"true",style:{"counter-reset":"line-number 0"}},[e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"})])],-1),ee=e("p",null,"To get all the records based on a duplicate count, you can pass the input file twice. Then use the two file processing trick to make decisions.",-1),se=e("p",null,"all duplicates based on the last column",-1),ne=e("div",{class:"language-bash line-numbers-mode","data-highlighter":"prismjs","data-ext":"sh","data-title":"sh"},[e("pre",null,[e("code",null,[e("span",{class:"line"},[e("span",{class:"token function"},"awk"),s(" -F, "),e("span",{class:"token string"},"'NR==FNR{a[$NF]++; next} a[$NF]>1'"),s(" duplicates.txt duplicates.txt")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# dark red,ruby,rose,111")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# blue,ruby,water,333")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# yellow,toy,flower,333")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# white,sky,bread,111")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# light red,purse,rose,333")]),s(`
`),e("span",{class:"line"})])]),e("div",{class:"line-numbers","aria-hidden":"true",style:{"counter-reset":"line-number 0"}},[e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"})])],-1),ae=e("p",null,"all duplicates based on the last column, minimum 3 duplicates",-1),le=e("div",{class:"language-bash line-numbers-mode","data-highlighter":"prismjs","data-ext":"sh","data-title":"sh"},[e("pre",null,[e("code",null,[e("span",{class:"line"},[e("span",{class:"token function"},"awk"),s(" -F, "),e("span",{class:"token string"},"'NR==FNR{a[$NF]++; next} a[$NF]>2'"),s(" duplicates.txt duplicates.txt")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# blue,ruby,water,333")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# yellow,toy,flower,333")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# light red,purse,rose,333")]),s(`
`),e("span",{class:"line"})])]),e("div",{class:"line-numbers","aria-hidden":"true",style:{"counter-reset":"line-number 0"}},[e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"})])],-1),ie=e("p",null,"only unique lines based on the third column",-1),te=e("div",{class:"language-bash line-numbers-mode","data-highlighter":"prismjs","data-ext":"sh","data-title":"sh"},[e("pre",null,[e("code",null,[e("span",{class:"line"},[e("span",{class:"token function"},"awk"),s(" -F, "),e("span",{class:"token string"},"'NR==FNR{a[$3]++; next} a[$3]==1'"),s(" duplicates.txt duplicates.txt")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# blue,ruby,water,333")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# yellow,toy,flower,333")]),s(`
`),e("span",{class:"line"})])]),e("div",{class:"line-numbers","aria-hidden":"true",style:{"counter-reset":"line-number 0"}},[e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"})])],-1),ce=d('<hr><h2 id="summary" tabindex="-1"><a class="header-anchor" href="#summary"><span>Summary</span></a></h2><p>This chapter showed how to work with duplicate contents for records and fields. If you don&#39;t need regexp based separators and if your input is too big to handle, then specialized command line tools like <code>sort</code> and <code>uniq</code> will be better suited compared to <code>awk</code>.</p><p>Next chapter will show how to write <code>awk</code> scripts instead of the usual one-liners.</p><hr><h2 id="exercises" tabindex="-1"><a class="header-anchor" href="#exercises"><span>Exercises</span></a></h2>',6),oe={class:"hint-container info"},re=e("p",{class:"hint-container-title"},"Info",-1),de={href:"https://github.com/learnbyexample/learn_gnuawk/tree/master/exercises",target:"_blank",rel:"noopener noreferrer"},pe=e("h3",{id:"exercise-1",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#exercise-1"},[e("span",null,"Exercise 1")])],-1),ue=e("code",null,"lines.txt",-1),me=e("code",null,"hi there",-1),he=e("code",null,"HI TheRE",-1),be=d(`<div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line"><span class="token function">cat</span> lines.txt</span>
<span class="line"><span class="token comment"># Go There</span></span>
<span class="line"><span class="token comment"># come on</span></span>
<span class="line"><span class="token comment"># go there</span></span>
<span class="line"><span class="token comment"># ---</span></span>
<span class="line"><span class="token comment"># 2 apples and 5 mangoes</span></span>
<span class="line"><span class="token comment"># come on!</span></span>
<span class="line"><span class="token comment"># ---</span></span>
<span class="line"><span class="token comment"># 2 Apples</span></span>
<span class="line"><span class="token comment"># COME ON</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,1),ve=e("div",{class:"language-bash line-numbers-mode","data-highlighter":"prismjs","data-ext":"sh","data-title":"sh"},[e("pre",null,[e("code",null,[e("span",{class:"line"},[e("span",{class:"token function"},"awk"),s(),e("span",{class:"token comment"},"##### add your solution here")]),s(`
`),e("span",{class:"line"},"Go There"),s(`
`),e("span",{class:"line"},"come on"),s(`
`),e("span",{class:"line"},"---"),s(`
`),e("span",{class:"line"},[e("span",{class:"token number"},"2"),s(" apples and "),e("span",{class:"token number"},"5"),s(" mangoes")]),s(`
`),e("span",{class:"line"},[s("come on"),e("span",{class:"token operator"},"!")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token number"},"2"),s(" Apples")]),s(`
`),e("span",{class:"line"})])]),e("div",{class:"line-numbers","aria-hidden":"true",style:{"counter-reset":"line-number 0"}},[e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"})])],-1),ke=e("div",{class:"language-bash line-numbers-mode","data-highlighter":"prismjs","data-ext":"sh","data-title":"sh"},[e("pre",null,[e("code",null,[e("span",{class:"line"},[e("span",{class:"token function"},"awk"),s(),e("span",{class:"token string"},"'!seen[tolower($0)]++'"),s(" lines.txt")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# Go There")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# come on")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# ---")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# 2 apples and 5 mangoes")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# come on!")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# 2 Apples")]),s(`
`),e("span",{class:"line"})])]),e("div",{class:"line-numbers","aria-hidden":"true",style:{"counter-reset":"line-number 0"}},[e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"})])],-1),ge=e("h3",{id:"exercise-2",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#exercise-2"},[e("span",null,"Exercise 2")])],-1),_e=e("code",null,"twos.txt",-1),fe=e("code",null,"hehe haha",-1),we=e("code",null,"haha hehe",-1),xe=d(`<div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line"><span class="token function">cat</span> twos.txt</span>
<span class="line"><span class="token comment"># hehe haha</span></span>
<span class="line"><span class="token comment"># door floor</span></span>
<span class="line"><span class="token comment"># haha hehe</span></span>
<span class="line"><span class="token comment"># 6;8 3-4</span></span>
<span class="line"><span class="token comment"># true blue</span></span>
<span class="line"><span class="token comment"># hehe bebe</span></span>
<span class="line"><span class="token comment"># floor door</span></span>
<span class="line"><span class="token comment"># 3-4 6;8</span></span>
<span class="line"><span class="token comment"># tru eblue</span></span>
<span class="line"><span class="token comment"># haha hehe</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,1),ye=e("div",{class:"language-bash line-numbers-mode","data-highlighter":"prismjs","data-ext":"sh","data-title":"sh"},[e("pre",null,[e("code",null,[e("span",{class:"line"},[e("span",{class:"token function"},"awk"),s(),e("span",{class:"token comment"},"##### add your solution here")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# hehe haha")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# door floor")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# 6;8 3-4")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# true blue")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# hehe bebe")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# tru eblue")]),s(`
`),e("span",{class:"line"})])]),e("div",{class:"line-numbers","aria-hidden":"true",style:{"counter-reset":"line-number 0"}},[e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"})])],-1),Ae=e("div",{class:"language-bash line-numbers-mode","data-highlighter":"prismjs","data-ext":"sh","data-title":"sh"},[e("pre",null,[e("code",null,[e("span",{class:"line"},[e("span",{class:"token function"},"awk"),s(),e("span",{class:"token string"},"'!($1,$2) in seen && !($2,$1) in seen; {seen[$1,$2]}'"),s(" twos.txt")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# hehe haha")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# door floor")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# 6;8 3-4")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# true blue")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# hehe bebe")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# tru eblue")]),s(`
`),e("span",{class:"line"})])]),e("div",{class:"line-numbers","aria-hidden":"true",style:{"counter-reset":"line-number 0"}},[e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"})])],-1),Ce=e("h3",{id:"exercise-3",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#exercise-3"},[e("span",null,"Exercise 3")])],-1),$e=e("code",null,"twos.txt",-1),Fe=e("code",null,"uniq.txt",-1),Ne=e("code",null,"dupl.txt",-1),Te=e("code",null,"hehe haha",-1),je=e("code",null,"haha hehe",-1),Se=e("div",{class:"language-bash line-numbers-mode","data-highlighter":"prismjs","data-ext":"sh","data-title":"sh"},[e("pre",null,[e("code",null,[e("span",{class:"line"},[e("span",{class:"token function"},"awk"),s(),e("span",{class:"token comment"},"##### add your solution here")]),s(`
`),e("span",{class:"line"}),s(`
`),e("span",{class:"line"},[e("span",{class:"token function"},"cat"),s(" uniq.txt ")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# true blue")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# hehe bebe")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# tru eblue")]),s(`
`),e("span",{class:"line"}),s(`
`),e("span",{class:"line"},[e("span",{class:"token function"},"cat"),s(" dupl.txt ")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# hehe haha")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# door floor")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# haha hehe")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# 6;8 3-4")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# floor door")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# 3-4 6;8")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# haha hehe")]),s(`
`),e("span",{class:"line"})])]),e("div",{class:"line-numbers","aria-hidden":"true",style:{"counter-reset":"line-number 0"}},[e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"})])],-1),Ee=e("div",{class:"language-bash line-numbers-mode","data-highlighter":"prismjs","data-ext":"sh","data-title":"sh"},[e("pre",null,[e("code",null,[e("span",{class:"line"},[e("span",{class:"token function"},"awk"),s(),e("span",{class:"token string"},`'NR==FNR{c[$1,$2]++; next} {if((c[$1,$2] + c[$2,$1]) == 1) print > "uniq.txt";`),s(`
`),e("span",{class:"line"},`     else print > "dupl.txt"}'`),s(" twos.txt twos.txt")]),s(`
`),e("span",{class:"line"}),s(`
`),e("span",{class:"line"},[e("span",{class:"token function"},"cat"),s(" uniq.txt ")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# true blue")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# hehe bebe")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# tru eblue")]),s(`
`),e("span",{class:"line"}),s(`
`),e("span",{class:"line"},[e("span",{class:"token function"},"cat"),s(" dupl.txt ")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# hehe haha")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# door floor")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# haha hehe")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# 6;8 3-4")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# floor door")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# 3-4 6;8")]),s(`
`),e("span",{class:"line"},[e("span",{class:"token comment"},"# haha hehe")]),s(`
`),e("span",{class:"line"})])]),e("div",{class:"line-numbers","aria-hidden":"true",style:{"counter-reset":"line-number 0"}},[e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"}),e("div",{class:"line-number"})])],-1),Re=e("hr",null,null,-1);function De(p,qe){const c=r("router-link"),u=r("VPCard"),o=r("FontIcon"),t=r("Tabs"),m=r("TagLinks");return _(),b("div",null,[e("h1",w,[e("a",x,[e("span",null,v(p.$frontmatter.title)+" 관련",1)])]),e("nav",y,[e("ul",null,[e("li",null,[i(c,{to:"#whole-line-duplicates"},{default:n(()=>[s("Whole line duplicates")]),_:1})]),e("li",null,[i(c,{to:"#column-wise-duplicates"},{default:n(()=>[s("Column wise duplicates")]),_:1})]),e("li",null,[i(c,{to:"#duplicate-count"},{default:n(()=>[s("Duplicate count")]),_:1})]),e("li",null,[i(c,{to:"#summary"},{default:n(()=>[s("Summary")]),_:1})]),e("li",null,[i(c,{to:"#exercises"},{default:n(()=>[s("Exercises")]),_:1}),e("ul",null,[e("li",null,[i(c,{to:"#exercise-1"},{default:n(()=>[s("Exercise 1")]),_:1})]),e("li",null,[i(c,{to:"#exercise-2"},{default:n(()=>[s("Exercise 2")]),_:1})]),e("li",null,[i(c,{to:"#exercise-3"},{default:n(()=>[s("Exercise 3")]),_:1})])])])])]),A,i(u,k(g({title:"13. Dealing with duplicates",desc:"CLI Text Processing with GNU awk",link:"https://learnbyexample.github.io/learn_gnuawk/dealing-with-duplicates.html",logo:"https://learnbyexample.github.io/favicon.svg",background:"rgba(22,25,35,0.2)"})),null,16),C,$,e("div",F,[N,e("p",null,[s("The "),e("a",T,[i(o,{icon:"iconfont icon-github"}),s(" example_files")]),s(" directory has all the files used in the examples.")])]),j,i(t,{id:"25",data:[{id:"Case 1"},{id:"Case 2"}],active:0},{title0:n(({value:a,isActive:l})=>[s("Case 1")]),title1:n(({value:a,isActive:l})=>[s("Case 2")]),tab0:n(({value:a,isActive:l})=>[S]),tab1:n(({value:a,isActive:l})=>[E,R]),_:1}),e("div",D,[q,e("p",null,[s("See also "),e("a",I,[i(o,{icon:"iconfont icon-github"}),s(" koraa/huniq")]),s(", a faster alternative for removing line based duplicates.")])]),P,i(t,{id:"49",data:[{id:"Case 1"}],active:0},{title0:n(({value:a,isActive:l})=>[s("Case 1")]),tab0:n(({value:a,isActive:l})=>[G,Q]),_:1}),U,i(t,{id:"60",data:[{id:"Case 1"}],active:0},{title0:n(({value:a,isActive:l})=>[s("Case 1")]),tab0:n(({value:a,isActive:l})=>[V,B]),_:1}),L,z,O,W,i(t,{id:"78",data:[{id:"Case 1"},{id:"Case 2"}],active:0},{title0:n(({value:a,isActive:l})=>[s("Case 1")]),title1:n(({value:a,isActive:l})=>[s("Case 2")]),tab0:n(({value:a,isActive:l})=>[K,Z]),tab1:n(({value:a,isActive:l})=>[J,M]),_:1}),H,i(t,{id:"95",data:[{id:"Case 1"}],active:0},{title0:n(({value:a,isActive:l})=>[s("Case 1")]),tab0:n(({value:a,isActive:l})=>[X,Y]),_:1}),ee,i(t,{id:"106",data:[{id:"Case 1"},{id:"Case 2"},{id:"Case 3"}],active:0},{title0:n(({value:a,isActive:l})=>[s("Case 1")]),title1:n(({value:a,isActive:l})=>[s("Case 2")]),title2:n(({value:a,isActive:l})=>[s("Case 3")]),tab0:n(({value:a,isActive:l})=>[se,ne]),tab1:n(({value:a,isActive:l})=>[ae,le]),tab2:n(({value:a,isActive:l})=>[ie,te]),_:1}),ce,e("div",oe,[re,e("p",null,[s("The "),e("a",de,[i(o,{icon:"iconfont icon-github"}),s(" exercises")]),s(" directory has all the files used in this section.")])]),pe,e("p",null,[s("Retain only the first copy of a line for the input file "),i(o,{icon:"fas fa-file-lines"}),s(),ue,s(". Case should be ignored while comparing the lines. For example, "),me,s(" and "),he,s(" should be considered as duplicates.")]),be,i(t,{id:"152",data:[{id:"Question"},{id:"Solution"}],active:0},{title0:n(({value:a,isActive:l})=>[s("Question")]),title1:n(({value:a,isActive:l})=>[s("Solution")]),tab0:n(({value:a,isActive:l})=>[ve]),tab1:n(({value:a,isActive:l})=>[ke]),_:1}),ge,e("p",null,[s("Retain only the first copy of a line for the input file "),i(o,{icon:"fas fa-file-lines"}),s(),_e,s(". Assume space as the field separator with exactly two fields per line. Compare the lines irrespective of the order of the fields. For example, "),fe,s(" and "),we,s(" should be considered as duplicates.")]),xe,i(t,{id:"167",data:[{id:"Question"},{id:"Solution"}],active:0},{title0:n(({value:a,isActive:l})=>[s("Question")]),title1:n(({value:a,isActive:l})=>[s("Solution")]),tab0:n(({value:a,isActive:l})=>[ye]),tab1:n(({value:a,isActive:l})=>[Ae]),_:1}),Ce,e("p",null,[s("For the input file "),i(o,{icon:"fas fa-file-lines"}),s(),$e,s(", create a file "),i(o,{icon:"fas fa-file-lines"}),s(),Fe,s(" with all the unique lines and "),i(o,{icon:"fas fa-file-lines"}),s(),Ne,s(" with all the duplicate lines. Assume space as the field separator with exactly two fields per line. Compare the lines irrespective of the order of the fields. For example, "),Te,s(" and "),je,s(" should be considered as duplicates.")]),i(t,{id:"181",data:[{id:"Question"},{id:"Solution"}],active:0},{title0:n(({value:a,isActive:l})=>[s("Question")]),title1:n(({value:a,isActive:l})=>[s("Solution")]),tab0:n(({value:a,isActive:l})=>[Se]),tab1:n(({value:a,isActive:l})=>[Ee]),_:1}),Re,i(m)])}const Ge=h(f,[["render",De],["__file","13-dealing-with-duplicates.html.vue"]]),Qe=JSON.parse('{"path":"/cli/text-processing-w-gnu-awk/13-dealing-with-duplicates.html","title":"13. Dealing with duplicates","lang":"ko-KR","frontmatter":{"lang":"ko-KR","title":"13. Dealing with duplicates","description":"Text Processing with GNU awk > 13. Dealing with duplicates","category":["CLI","Linux"],"tags":["crashcourse","cli","sh","shell","gnu","linux","awk"],"head":[[{"meta":null},{"property":"og:title","content":"Text Processing with GNU awk > 13. Dealing with duplicates"},{"property":"og:description","content":"13. Dealing with duplicates"},{"property":"og:url","content":"https://chanhi2000.github.io/crashcourse/cli/text-processing-w-gnu-awk/13-dealing-with-duplicates.html"}],["meta",{"property":"og:url","content":"https://chanhi2000.github.io/crashcourse/crashcourse/cli/text-processing-w-gnu-awk/13-dealing-with-duplicates.html"}],["meta",{"property":"og:site_name","content":"🥁Crashcourse"}],["meta",{"property":"og:title","content":"13. Dealing with duplicates"}],["meta",{"property":"og:description","content":"Text Processing with GNU awk > 13. Dealing with duplicates"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"ko-KR"}],["meta",{"property":"og:updated_time","content":"2024-04-24T07:16:44.000Z"}],["meta",{"property":"article:tag","content":"crashcourse"}],["meta",{"property":"article:tag","content":"cli"}],["meta",{"property":"article:tag","content":"sh"}],["meta",{"property":"article:tag","content":"shell"}],["meta",{"property":"article:tag","content":"gnu"}],["meta",{"property":"article:tag","content":"linux"}],["meta",{"property":"article:tag","content":"awk"}],["meta",{"property":"article:modified_time","content":"2024-04-24T07:16:44.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"13. Dealing with duplicates\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2024-04-24T07:16:44.000Z\\",\\"author\\":[]}"]]},"headers":[{"level":2,"title":"Whole line duplicates","slug":"whole-line-duplicates","link":"#whole-line-duplicates","children":[]},{"level":2,"title":"Column wise duplicates","slug":"column-wise-duplicates","link":"#column-wise-duplicates","children":[]},{"level":2,"title":"Duplicate count","slug":"duplicate-count","link":"#duplicate-count","children":[]},{"level":2,"title":"Summary","slug":"summary","link":"#summary","children":[]},{"level":2,"title":"Exercises","slug":"exercises","link":"#exercises","children":[{"level":3,"title":"Exercise 1","slug":"exercise-1","link":"#exercise-1","children":[]},{"level":3,"title":"Exercise 2","slug":"exercise-2","link":"#exercise-2","children":[]},{"level":3,"title":"Exercise 3","slug":"exercise-3","link":"#exercise-3","children":[]}]}],"git":{"createdTime":1703642364000,"updatedTime":1713943004000,"contributors":[{"name":"chanhi2000","email":"chanhi2000@gmail.com","commits":6}]},"readingTime":{"minutes":4.3,"words":1290},"filePathRelative":"cli/text-processing-w-gnu-awk/13-dealing-with-duplicates.md","localizedDate":"2023년 12월 27일","excerpt":"\\n\\n<hr>\\n"}');export{Ge as comp,Qe as data};
